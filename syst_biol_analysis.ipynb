{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52fde16e-2eb9-40a1-a169-dc2d155c17a8",
   "metadata": {},
   "source": [
    "# Analysis code\n",
    "\n",
    "Used for <b>Reflections on 75 Momentous Years of *Systematic Biology*</b> by Landis and Donoghue\n",
    "\n",
    "This Python code does the following:\n",
    "- Cleans up and extracts relevant articles from *SZ*/*SB* dataset\n",
    "- Embeds research articles using SPECTER\n",
    "- Predicts topics using BERTopic\n",
    "- Measures similarity among articles\n",
    "- Measures citation rates for article clusters\n",
    "- Measures numbers and lengths of article types\n",
    "- Generates Figures 1-3 and Supp. Figures S1-5\n",
    "- Also generates some fun visualizations that didn't go in the paper\n",
    "- Saves topic modeling output to file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0494bc5-d161-4609-9b92-6f6e3907f583",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9829a22a-0988-4cdd-aa0f-2cb4dd023fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basics\n",
    "import datetime\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "# processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# plotting\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib import cm\n",
    "import plotly.graph_objects as go\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# machine learning stuff\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.cluster import hierarchy as sch\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy import stats\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "# BERT stuff\n",
    "from transformers import AutoModel\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer, OnlineCountVectorizer\n",
    "from bertopic.dimensionality import BaseDimensionalityReduction\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, ZeroShotClassification, PartOfSpeech\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fdf917-ebeb-4592-8b7f-ed18dc5a4627",
   "metadata": {},
   "source": [
    "## Functions, global vars, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a30f69-0889-4fab-a48c-2278fbf494d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_fraction(date):\n",
    "    start = datetime.date(date.year, 1, 1).toordinal()\n",
    "    year_length = datetime.date(date.year+1, 1, 1).toordinal() - start\n",
    "    return date.year + float(date.toordinal() - start) / year_length\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "def similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2)\n",
    "\n",
    "def moving_average(a, n=3):\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return np.concatenate( [a[:(n-1)], ret[n - 1:] / n] )\n",
    "\n",
    "def make_better_labels(topics_info, labels, df_counts, n=6):\n",
    "    tmp         = topics_info[ topics_info['Name'].isin(labels) ]\n",
    "    id          = tmp['Topic'].to_list()\n",
    "    num_topics  = topics_info.shape[0] - 1   #  remove outlier topic\n",
    "    count       = [ sum(df_counts[i].to_list()) for i in id ]\n",
    "    lbl         = [ f'Topic {id[i]}, n={count[i]}: ' + ', '.join(x[:n]) for i,x in enumerate(tmp['Representation']) ]\n",
    "    return lbl\n",
    "\n",
    "# custom colors\n",
    "my_colors = { 'orange':'#ffaa00',\n",
    "              'red':'#ee6677',\n",
    "              'blue':'#4477aa' }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2a5fa5-3686-4fe5-9d81-a84a303e49e6",
   "metadata": {},
   "source": [
    "## Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58385642-d2a8-4afe-8775-b774092c68a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILESYSTEM\n",
    "\n",
    "# base_dir     = \"/Users/mlandis/projects/syst_biol_75/\"\n",
    "base_dir     = \"./\"\n",
    "data_dir     = base_dir + \"data/\"\n",
    "plot_dir     = base_dir + \"plot/\"\n",
    "out_dir      = base_dir + \"output/\"\n",
    "papers_fn    = data_dir + \"systbiol_articles_enhanced_250717.xlsx\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57fdbb0-3abd-4e0d-999b-f19cda5feddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ FULL DATASET\n",
    "\n",
    "# get papers\n",
    "df_orig = pd.read_excel(papers_fn)\n",
    "\n",
    "# reverse order (oldest to youngest)\n",
    "df_orig = df_orig.iloc[::-1]\n",
    "\n",
    "# get statistics for original dataset\n",
    "num_records_orig = df_orig.shape[0]\n",
    "counts           = df_orig.groupby('PubYear').count()['DOI'].to_list()\n",
    "years_unique     = np.unique( df_orig['PubYear'] )\n",
    "\n",
    "# get all unique document types\n",
    "# NB: curated document types were updated based on Syst Biol website HTML + manual corrections\n",
    "df_orig['Document.Type.Curated'] = df_orig['Document.Type.Curated'].str.title()\n",
    "uniq_doc_types = df_orig['Document.Type.Curated'].unique()\n",
    "\n",
    "# find symposia document types (descriptions vary depending on symposium)\n",
    "symp_types = [ x for x in uniq_doc_types if 'Symposium' in x ]\n",
    "symp_types = [ x for x in symp_types if (x != 'Symposium Announcement' and x != 'Symposium Announcements')  ]\n",
    "\n",
    "# define set of articles to compare (we focus on research articles)\n",
    "# note: POVs and book reviews do not have abstracts\n",
    "valid_types = ['Research Article',]\n",
    "               # 'Point Of View',\n",
    "               # 'Book Review',\n",
    "               # 'Software For Systematics And Evolution',\n",
    "               # 'Spotlight Article'\n",
    "               # + symp_types\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230131b5-5f85-46fd-bfa9-525ecdfe0126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTERING + NEW INPUT COLUMNS\n",
    "\n",
    "# only retain records with valid curated document type\n",
    "df = df_orig[ df_orig['Document.Type.Curated'].isin(valid_types) ].copy()\n",
    "\n",
    "# text for each record represented by concatenation of abstract and title\n",
    "col_name                         = 'Title.Abstract'\n",
    "titles                           = df['Title'].astype(str)\n",
    "abstracts                        = df['Abstract'].astype(str)\n",
    "titles[ titles == 'nan' ]        = ''\n",
    "abstracts[ abstracts == 'nan' ]  = ''\n",
    "\n",
    "# '{title} [SEP] {abstract}' format should match representation used by SPECTER\n",
    "df['Title.Abstract']             = titles + '.  '  + '[SEP]' + '   ' + abstracts\n",
    "texts                            = df['Title.Abstract'].tolist()\n",
    "years                            = df['PubYear'].to_numpy(dtype='int')\n",
    "n_texts                          = len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbade519-dad3-4652-893b-e6d12f040e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESS ALTERNATIVE REPRESENTATIONS FOR TIMES\n",
    "\n",
    "# alternative representations of time based on publication date\n",
    "times_str = df['Publication.date'].tolist()\n",
    "times = []\n",
    "times_idx = []\n",
    "for i in range(len(times_str)):\n",
    "\n",
    "    # use PubYear if Publication.date info is absent\n",
    "    if times_str[i] != times_str[i]:\n",
    "        zz = pd.Timestamp(year=years[i], month=1, day=1)\n",
    "\n",
    "    # extract day/month if available\n",
    "    elif type(times_str[i]) == str:\n",
    "        dd = [ int(x) for x in times_str[i].split('-') ]\n",
    "        m = 1\n",
    "        d = 1\n",
    "        y = dd[0]\n",
    "        if len(dd) > 1:\n",
    "            m = dd[1]\n",
    "        if len(dd) > 2:\n",
    "            d = dd[2]\n",
    "        zz = datetime.date(y,m,d)    \n",
    "\n",
    "    # already correct format?\n",
    "    else:    \n",
    "        zz = times_str[i]\n",
    "        \n",
    "    times.append( year_fraction(zz) )\n",
    "    times_idx.append( zz.toordinal() )\n",
    "\n",
    "times_idx = np.array(times_idx, dtype='int')\n",
    "times_idx = times_idx - min(times_idx)\n",
    "\n",
    "# put new values back into dataframe\n",
    "df['TimesFraction']      = times\n",
    "df['AgesFraction']       = 2025 - df['TimesFraction']\n",
    "df['CitationsPerYear']   = df['Google.Scholar.Times.cited'] / df['AgesFraction']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5d8a05-8d46-4abf-8f94-4edc8de1b74d",
   "metadata": {},
   "source": [
    "## Topic modeling (processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3626957c-45d4-41f9-9d24-3ce2924c9d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE EMBEDDINGS\n",
    "\n",
    "# set random seed for reproducibility\n",
    "seed = 75\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# generate initial embeddings\n",
    "# link: https://www.sbert.net/docs/sentence_transformer/pretrained_models.html\n",
    "#model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')              # faster, generic\n",
    "#model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')             # higher quality, generic\n",
    "sentence_model = SentenceTransformer('sentence-transformers/allenai-specter')       # higher quality, scientific (best available)\n",
    "\n",
    "# transform texts into embedded vector space\n",
    "embeddings = sentence_model.encode(texts, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a05846-2b1d-433d-8846-3712b548fc21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GENERATE HIERARCHICAL TOPICS\n",
    "\n",
    "# BERTopic workflow is:\n",
    "\n",
    "# 1. Embedding: put text into 784-dim numerical space (computed above; slow)\n",
    "# 2. Dimension reduction: reduce embedding to k dimensions\n",
    "# 3. Clustering: infer unnamed topic clusters\n",
    "# 4. Vectorize: converts text into bag-of-words (vector) representation\n",
    "# 5. c-TF-IDF (topic-naming): extract the most important words for each topic\n",
    "# 6. Representation: refines topic keywords to make them more interpretable\n",
    "\n",
    "# Steps 1-3 determine the number and composition of topics, whereas steps 4-6\n",
    "# determine the keywords associated with each topic-cluster.\n",
    "\n",
    "# RNG\n",
    "seed = 75\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# good settings\n",
    "n_neighbors        = 20      # Dimension reduction\n",
    "n_components       = 5       # Dimension reduction\n",
    "min_dist           = 0.0     # Dimension reduction\n",
    "min_cluster_size   = 15      # Clustering\n",
    "min_samples        = 3       # Clustering\n",
    "seed_multiplier    = 2.0     # c-TF-IDF\n",
    "diversity          = 0.5     # Representation\n",
    "top_n_words_rep    = 10      # Representation\n",
    "top_n_words_full   = 30      # BERTopic\n",
    "n_gram_range       = (1,3)   # BERTopic\n",
    "min_topic_size     = 15      # BERTopic\n",
    "nr_topics          = 40      # BERTopic\n",
    "\n",
    "\n",
    "## 1. Embedding model:\n",
    "## - convert documents into k-dimensional vector space\n",
    "# ... pre-computed above!\n",
    "\n",
    "\n",
    "## 2. Dimension reduction model:\n",
    "## - project 784-dimensional vectors onto fewer dimensions\n",
    "# UMAP model pre-process reduce dimensionality of data down to n_components\n",
    "# umap_model = empty_dimensionality_model = BaseDimensionalityReduction()\n",
    "# umap_model = UMAP(n_neighbors=10, n_components=20, min_dist=0.0, metric='cosine', random_state=seed)\n",
    "umap_model = UMAP(n_neighbors=n_neighbors,\n",
    "                  n_components=n_components,\n",
    "                  min_dist=min_dist,\n",
    "                  metric='euclidean',\n",
    "                  random_state=seed)\n",
    "\n",
    "\n",
    "## 3. Clustering model:\n",
    "## - find dense clusters of documents in vector-space\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size,\n",
    "                        min_samples=min_samples,\n",
    "                        metric='euclidean',\n",
    "                        cluster_selection_method='eom',\n",
    "                        prediction_data=True)\n",
    "\n",
    "\n",
    "## 4. Vectorizer model: converts raw text into sparse matrix of word counts\n",
    "## - c-TF-IDF uses this matrix to choose topic names\n",
    "# vectorizer_model = OnlineCountVectorizer(stop_words=\"english\", decay=0.1)\n",
    "vectorizer_model =  CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "\n",
    "## 5. c-TF-IDF model:\n",
    "## - select which topics are most representative of papers\n",
    "# seed words\n",
    "# link: https://maartengr.github.io/BERTopic/getting_started/seed_words/seed_words.html\n",
    "seed_topic_list = ['phylogenetic','phylogeny','lineage',\n",
    "                   'taxonomy','taxon','species','delimitation','classification','nomenclature',\n",
    "                   'cladistic','clade','synapomorphy','homoplasy',\n",
    "                   'phenetic', 'numerical',\n",
    "                   'homology','ortholog','paralog',\n",
    "                   'divergence','dating','calibration','age','deep time',\n",
    "                   'macroevolution','microevolution',\n",
    "                   'topology','branch length','branch','mrca','treespace','tree distance',\n",
    "                   'simulation','estimation','inference','estimate','method','software',\n",
    "                   'likelihood','parsimony','bayesian','posterior','prior',\n",
    "                   'ancestral','rate','state','process','model','evolution',\n",
    "                   'monocot','dicot','plant','angiosperm','gymnosperm','fungi','woody','herbaceous','flower',\n",
    "                   'animal','mammal','bird','reptile','fish','amphibian','invertebrate','insect','arachnid','bivalve','mollusk','worm',\n",
    "                   'bacteria','virus','microbe','prokaryote','archaea','protist','yeast','mushroom',\n",
    "                   'nucleotide','genomic','genetic','adaptation','dna','rna','sequence',\n",
    "                   'mtdna','mitochondria','cpdna','chloroplast','plastid','electrophoretic','microsatellite',\n",
    "                   'epidemiology','infectious','outbreak','disease','phylodynamic',\n",
    "                   'phenotype','morphology','character','trait','biogeography','ecology','environment','niche',\n",
    "                   'speciation','diversification','radiation','fossilization','fossil','paleontology','paleobiology',\n",
    "                   'parasite','host','mutualism','mimicry','pollinator','symbiont','coevolution','cophylogenetic','codiversification',\n",
    "                   'hybridization','introgression','polyploidy','diploid','duplication','chromosome',\n",
    "                   'coalescent','substitution','concatenation','species tree','gene tree','network']\n",
    "\n",
    "# rebalance term frequency-inverse document frequency, reweight terms, import seed words\n",
    "# link: https://maartengr.github.io/BERTopic/getting_started/ctfidf/ctfidf.html\n",
    "ctfidf_model = ClassTfidfTransformer(seed_multiplier=seed_multiplier,\n",
    "                                     seed_words=seed_topic_list,\n",
    "                                     reduce_frequent_words=True,\n",
    "                                     bm25_weighting=True)\n",
    "\n",
    "\n",
    "\n",
    "## 6. Representation model:\n",
    "## - adjusts keywords proposed by c-TF-IDF to be more interpretable/diverse\n",
    "## - MMR: increases diversity of keywords\n",
    "\n",
    "mmr = MaximalMarginalRelevance(diversity=diversity,\n",
    "                               top_n_words=top_n_words_rep)\n",
    "## - POS: uses grammatical rules to combine keywords\n",
    "pos_patterns = [\n",
    "    [{'POS': 'ADJ'}, {'POS': 'NOUN'}],\n",
    "    [{'POS': 'NOUN'}],\n",
    "    [{'POS': 'ADJ'}]\n",
    "]\n",
    "pos = PartOfSpeech(\"en_core_web_lg\",\n",
    "                   top_n_words=top_n_words_rep,\n",
    "                   pos_patterns=pos_patterns)\n",
    "\n",
    "# setup vector of rep. models\n",
    "representation_model = [ mmr, pos ]\n",
    "\n",
    "\n",
    "## BERTopic analysis\n",
    "\n",
    "# construct topic model\n",
    "topic_model = BERTopic(\n",
    "                       embedding_model=sentence_model,\n",
    "                       representation_model=representation_model,\n",
    "                       ctfidf_model=ctfidf_model,\n",
    "                       umap_model=umap_model,\n",
    "                       vectorizer_model=vectorizer_model,\n",
    "                       hdbscan_model=hdbscan_model,\n",
    "                       verbose=True,\n",
    "                       top_n_words=top_n_words_full,\n",
    "                       n_gram_range=n_gram_range,\n",
    "                       min_topic_size=min_topic_size,\n",
    "                       nr_topics=nr_topics,\n",
    "                       calculate_probabilities=True)\n",
    "\n",
    "\n",
    "# get topics (and probs) for all texts\n",
    "topics, probs = topic_model.fit_transform(texts, embeddings)\n",
    "\n",
    "# construct hierarchically clustered embeddings\n",
    "print(\"Get hierarchical topics\")\n",
    "hierarchical_topics = topic_model.hierarchical_topics(texts)\n",
    "\n",
    "# approximate topic probabilities per token per document (takes time)\n",
    "print(\"Get topic probabilities per token per document\")\n",
    "topic_distr, topic_token_distr = topic_model.approximate_distribution(texts, calculate_tokens=True, window=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0d2477-8702-4bb8-a822-b3be5747a7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POSTPROCESS TOPIC ASSIGNMENTS\n",
    "\n",
    "probability_threshold  = 0.01\n",
    "new_topics             = [np.argmax(prob) if max(prob) >= probability_threshold else -1 for prob in probs]\n",
    "topics_bak             = topics\n",
    "topics                 = new_topics\n",
    "num_topics_old         = len(np.unique(topics_bak))\n",
    "num_topics             = len(np.unique(topics))\n",
    "num_assign_old         = sum(x != -1 for x in topics_bak)\n",
    "num_assign_new         = sum(x != -1 for x in new_topics)\n",
    "\n",
    "print('num_texts           =', len(topics))\n",
    "print('num_topics_old      =', num_topics_old)\n",
    "print('num_topics_new      =', num_topics)\n",
    "print('num_assign_old      =', num_assign_old)\n",
    "print('num_assign_new      =', num_assign_new)\n",
    "print('num_unassigned_old  =', len(topics) - num_assign_old)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b63c016-8f39-432e-bba4-b9369afb9c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE DATAFRAMES WITH POSTPROCESSED TOPICS\n",
    "\n",
    "# generate list of texts, updated topics, and topic probabilities\n",
    "text_topics                         = topic_model.get_document_info(texts)\n",
    "text_topics.insert(0, 'Research_Article_ID', range(text_topics.shape[0]))\n",
    "text_topics.insert(3, 'BestTopic', new_topics)\n",
    "prob_col_names                      = [ f'Prob_Topic_{i}' for i in range(num_topics) ]#\n",
    "text_topics[prob_col_names]         = probs\n",
    "\n",
    "# generate list of topics and updated topic counts\n",
    "topics_info                         = topic_model.get_topic_info()\n",
    "new_topic_counts                    = dict.fromkeys(topics_info['Topic'], 0)\n",
    "for id in new_topics:\n",
    "    new_topic_counts[id]            = new_topic_counts[id] + 1\n",
    "    \n",
    "topics_info.insert(2, 'BestTopicCount', new_topic_counts.values())\n",
    "topics_info.insert(3, 'BestTopicCountRatioIncrease', topics_info['BestTopicCount'] / topics_info['Count'])\n",
    "\n",
    "# update main datafarme with updated topics\n",
    "df['TopicNumber']                   = topics\n",
    "df['TopicLabel']                    = [ topics_info['Name'][x+1] for x in topics ]\n",
    "\n",
    "# save to file\n",
    "topics_info.to_csv(out_dir + 'topics_info.csv')\n",
    "text_topics.to_csv(out_dir + 'texts_info.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8781508-9f53-4bc6-8e05-e3ae3d439d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIND DEEP CLUSTERS\n",
    "\n",
    "# distance threshold:\n",
    "# - larger distance/depth means fewer, larger clusters\n",
    "# - smaller distance/depth means more, smaller clusters\n",
    "clust_dist = 1.1    \n",
    "\n",
    "# get all cluster-nodes with distance > dist\n",
    "cluster_nodes = hierarchical_topics[ hierarchical_topics['Distance'] > clust_dist ]\n",
    "cluster_id = cluster_nodes['Child_Left_ID'].to_list() + cluster_nodes['Child_Right_ID'].to_list()\n",
    "cluster_id = [ x for x in cluster_id if x not in cluster_nodes['Parent_ID'].to_list() ]\n",
    "\n",
    "# info about deep clusters\n",
    "# print(cluster_nodes)\n",
    "# print(cluster_id)\n",
    "\n",
    "# store topics for deep-cluster nodes\n",
    "deep_topic_id = {}\n",
    "df['deep_topic_id'] = 0\n",
    "for i in cluster_id:\n",
    "    deep_topic_id[i] = hierarchical_topics[ hierarchical_topics['Parent_ID']==i ].iloc[0]['Topics']\n",
    "    match_idx = df['TopicNumber'].isin( deep_topic_id[i] )\n",
    "    df['deep_topic_id'][match_idx] = i\n",
    "\n",
    "# verify no topics are lost\n",
    "num_topics_deep = sum( [ len(x) for x in deep_topic_id.values()  ] )\n",
    "num_topics_flat = len(np.unique(topics))\n",
    "if num_topics_deep != num_topics_flat:\n",
    "    print(f'num_topics_deep = {num_topics_deep}')\n",
    "    print(f'num_topics_flat = {num_topics_flat}')\n",
    "\n",
    "# get numbers of papers per topic per year\n",
    "df_ttt = pd.DataFrame( {'year':years, 'topic':new_topics} )\n",
    "df_counts = df_ttt.groupby(['topic','year']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a535e8bf-ceea-44fa-9175-0e545d7df792",
   "metadata": {},
   "source": [
    "## Topic modeling (visualizations)\n",
    "\n",
    "Plots are generated in a different order than how they appear in the paper, since they need to share information in particular ways (e.g. colors of topics, designation of clusters, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b04e3a3-bb6c-4a58-afe6-c5432fc88321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIGURE S2: PLOT TOPIC TREE & MAIN CLUSTERS\n",
    "\n",
    "fig = topic_model.visualize_hierarchy(color_threshold=clust_dist, title='')\n",
    "\n",
    "# customize figure\n",
    "fig['layout']['title']                 = None\n",
    "fig['layout']['xaxis']['automargin']   = True\n",
    "fig['layout']['yaxis']['automargin']   = True\n",
    "fig['layout']['yaxis']['side']         = 'right'\n",
    "fig['layout']['paper_bgcolor']         = 'rgba(0,0,0,0)'\n",
    "fig['layout']['plot_bgcolor']          = 'rgba(0,0,0,0)'\n",
    "fig['layout']['xaxis']['showline']     = False\n",
    "fig['layout']['xaxis']['ticks']        = 'outside'\n",
    "fig['layout']['xaxis']['side']         = 'bottom' \n",
    "fig['layout']['xaxis']['mirror']       = False\n",
    "fig['layout']['yaxis']['showline']     = False\n",
    "fig['layout']['yaxis']['ticks']        = ''\n",
    "\n",
    "# compute range of data\n",
    "all_x = []\n",
    "all_y = []\n",
    "for trace in fig['data']:\n",
    "    if 'x' in trace and trace['x'] is not None:\n",
    "        all_x.extend(trace['x'])\n",
    "    if 'y' in trace and trace['y'] is not None:\n",
    "        all_y.extend(trace['y'])\n",
    "\n",
    "# manually reverse axes against data range\n",
    "# ... did this instead of auto reverse because of padding issues\n",
    "def make_rev_axis(z, padding=0.01):\n",
    "    zmin = min(z)\n",
    "    zmax = max(z)\n",
    "    zpad = padding * (zmax - zmin)\n",
    "    return [zmax + zpad, zmin - zpad]\n",
    "\n",
    "fig['layout']['xaxis']['range'] = make_rev_axis(all_x)\n",
    "fig['layout']['yaxis']['range'] = make_rev_axis(all_y)\n",
    "\n",
    "# update colors\n",
    "for i in range(len(fig['data'])):\n",
    "    if fig['data'][i]['marker']['color'] == 'rgb(61,153,112)':\n",
    "        fig['data'][i]['line']['color'] = my_colors['blue']\n",
    "    elif fig['data'][i]['marker']['color'] == 'rgb(255,65,54)':\n",
    "        fig['data'][i]['line']['color'] = my_colors['red']\n",
    "    elif fig['data'][i]['marker']['color'] == 'rgb(35,205,205)':\n",
    "        fig['data'][i]['line']['color'] = my_colors['orange']\n",
    "    elif fig['data'][i]['marker']['color'] == 'rgb(0,116,217)':\n",
    "        fig['data'][i]['line']['color'] = 'black'\n",
    "\n",
    "# save figure\n",
    "fig.write_image(plot_dir + \"fig_S2_topic_tree.pdf\", format=\"pdf\", height=800, width=800)  # requires kaleido\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa75ca8-2300-4b83-a84b-ea8bf5f602f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# FIGURE 2: PLOT TOPICS THROUGH TIME BY CLUSTER\n",
    "\n",
    "n_cluster = len(deep_topic_id)\n",
    "if n_cluster == 1:\n",
    "    n_cluster = 2\n",
    "\n",
    "titles = {}\n",
    "for i in range(n_cluster):\n",
    "    titles[i] = f'Cluster {i+1} research articles'\n",
    "    \n",
    "# figure\n",
    "fig, ax = plt.subplots( nrows=n_cluster, ncols=2, width_ratios=[0.7, 1])\n",
    "fig.set_size_inches(15, 5*n_cluster)\n",
    "fig.tight_layout(pad=4.0)\n",
    "\n",
    "# display\n",
    "colormap1 = mpl.colormaps['tab20b'] #, len(np.unique(ttt_right['Topic'])))\n",
    "colormap2 = mpl.colormaps['tab20c'] #, len(np.unique(ttt_right['Topic'])))\n",
    "colors = np.vstack( [colormap1(range(20)), colormap2(range(20))] )\n",
    "color_idx = 0\n",
    "font_size = 10\n",
    "\n",
    "# get vector of colors for each topic id\n",
    "color_id_vec = [None] * num_topics\n",
    "\n",
    "# get data\n",
    "df_count_times = np.unique( [ x[1] for x in df_counts.index ] )\n",
    "for i,(k,v) in enumerate(reversed(deep_topic_id.items())):\n",
    "    y_val = {}\n",
    "    df_count_id = np.unique( [ x[0] for x in df_counts.index ] )\n",
    "    ii = 0\n",
    "\n",
    "    for id in df_count_id:\n",
    "        df_id = df_counts[id,:]\n",
    "        # skip outlier-cluster\n",
    "        if id == -1:\n",
    "            continue\n",
    "        # skip any topic ID not in the cluster\n",
    "        if id not in v:\n",
    "            continue\n",
    "\n",
    "        # initialize each topic for zero papers each year\n",
    "        y_dict = {}\n",
    "        for t in df_count_times:\n",
    "            y_dict[t] = 0.0\n",
    "\n",
    "        # update number of papers for recorded years\n",
    "        for j,ct in enumerate(df_id):\n",
    "            year = float(df_id.index[j])\n",
    "            y_dict[ year ] = ct\n",
    "\n",
    "        # update y_val with papers per year per topic \n",
    "        kk = topics_info['Name'][id+1]\n",
    "        yyy = moving_average( list(y_dict.values()), 4)\n",
    "        y_val[kk] = np.array(yyy, dtype=float)\n",
    "\n",
    "        \n",
    "        #color_id_dict[id] = colors[color_idx:][ii]\n",
    "        ii += 1\n",
    "\n",
    "    \n",
    "    ax[i,0].stackplot(df_count_times, list(y_val.values())[::-1], labels=y_val.keys(), colors=colors[color_idx:])\n",
    "\n",
    "    # add color for topic id\n",
    "    for j,id in enumerate(reversed(v)):\n",
    "        color_id_vec[id] = colors[color_idx:][j]\n",
    "    \n",
    "    color_idx = color_idx + len(v)\n",
    "    ax[i,0].set_ylim([0,40])\n",
    "    handles, labels = ax[i,0].get_legend_handles_labels()\n",
    "\n",
    "    better_labels = make_better_labels(topics_info, labels, df_counts, 7)\n",
    "    \n",
    "    ax[i,0].legend(handles[::-1], better_labels, loc='upper left', bbox_to_anchor=(1.04, 1), fontsize=font_size)\n",
    "    ax[i,0].set_title(titles[i])\n",
    "    ax[i,0].set_xlabel('Year')\n",
    "    ax[i,0].set_ylabel('Num. articles')\n",
    "    ax[i,1].axis('off')\n",
    "\n",
    "    print(f'Cluster {i+1} size = ', np.sum(np.array(list(y_val.values()))), f'\\tnum_topics = {num_topics}')\n",
    "\n",
    "# convert color_id to color_vec\n",
    "# used for figures S1 and S3!\n",
    "color_id = np.vstack(color_id_vec)\n",
    "\n",
    "# save figure\n",
    "fig.savefig(plot_dir + 'fig_2_topic_thru_time_clusters.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6853f9-c2d1-43e0-ae7c-33d43e04aa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIGURE S3: PLOT TOPICS THROUGH TIME FOR ALL ARTICLES\n",
    "\n",
    "# figure\n",
    "fig, ax = plt.subplots( nrows=1, ncols=2, width_ratios=[3, 1])\n",
    "fig.set_size_inches(14,8)\n",
    "fig.tight_layout(pad=4.0)\n",
    "\n",
    "# display\n",
    "colormap1 = mpl.colormaps['tab20b']\n",
    "colormap2 = mpl.colormaps['tab20c']\n",
    "colors = np.vstack( [colormap1(range(20)), colormap2(range(20))] )\n",
    "color_idx = 0\n",
    "font_size = 7\n",
    "\n",
    "# get data\n",
    "df_count_times = np.unique( [ x[1] for x in df_counts.index ] )\n",
    "\n",
    "i = 0\n",
    "y_val = {}\n",
    "df_count_id = np.unique( [ x[0] for x in df_counts.index ] )\n",
    "for id in df_count_id:\n",
    "    df_id = df_counts[id,:]\n",
    "\n",
    "    # skip outlier-cluster\n",
    "    if id == -1:\n",
    "        continue\n",
    "\n",
    "    # initialize each topic for zero papers each year\n",
    "    y_dict = {}\n",
    "    for t in df_count_times:\n",
    "        y_dict[t] = 0.0\n",
    "\n",
    "    # update number of papers for recorded years\n",
    "    for j,ct in enumerate(df_id):\n",
    "        year = float(df_id.index[j])\n",
    "        y_dict[ year ] = ct\n",
    "\n",
    "    # update y_val with papers per year per topic \n",
    "    kk = topics_info['Name'][id+1]\n",
    "    yyy = moving_average( list(y_dict.values()), 4)\n",
    "    y_val[kk] = np.array(yyy, dtype=float)\n",
    "\n",
    "num_topics = len(list(y_val.keys()))\n",
    "\n",
    "ax[0].stackplot(df_count_times, list(y_val.values()), labels=y_val.keys(), colors=color_id[:])\n",
    "color_idx = color_idx + num_topics\n",
    "ax[0].set_ylim([0,65])\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "ax[0].legend(handles[::], labels[::], loc='upper center',\n",
    "             bbox_to_anchor=(0.5, -0.075), frameon=False, fontsize=font_size, ncol=3)\n",
    "\n",
    "ax[0].set_title('All research articles')\n",
    "ax[0].set_xlabel('Year')\n",
    "ax[0].set_ylabel('Num. articles')\n",
    "ax[1].axis('off')\n",
    "\n",
    "print(f'Cluster {i} size = ', np.sum(np.array(list(y_val.values()))), f'\\tnum_topics = {num_topics}')\n",
    "\n",
    "# save figure\n",
    "fig.savefig(plot_dir + 'fig_S3_topic_thru_time_all.pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad37ef8-0ecc-463e-a0c3-7e926ab6d885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIGURE S1: PLOT TOPICS ON 2D UMAP SPACE\n",
    "\n",
    "reduced_embeddings_2d = UMAP(n_neighbors=20, n_components=2, metric='euclidean', min_dist=0.0).fit_transform(embeddings)\n",
    "\n",
    "better_labels = make_better_labels(topics_info, topics_info['Name'].iloc[1:], df_counts, n=7)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(17,13))\n",
    "\n",
    "# topic_colors = color_id[ topics,: ]\n",
    "\n",
    "# plot articles\n",
    "for id in range(num_topics):\n",
    "    id_idx = np.where( np.array(topics) == id)[0]\n",
    "    x = reduced_embeddings_2d[id_idx,0]\n",
    "    y = reduced_embeddings_2d[id_idx,1]\n",
    "    ax.scatter(x=x, y=y, c=color_id[id,:], s=16, marker='o', label=better_labels[id], alpha=1)\n",
    "\n",
    "# plot topic identifiers\n",
    "for id in range(num_topics):    \n",
    "    id_idx = np.where( np.array(topics) == id)[0]\n",
    "    x = reduced_embeddings_2d[id_idx,0]\n",
    "    y = reduced_embeddings_2d[id_idx,1]\n",
    "    mean_x = np.median(x)\n",
    "    mean_y = np.median(y)\n",
    "    ax.scatter(x=mean_x, y=mean_y,\n",
    "               facecolor='white', edgecolor=color_id[id,:],\n",
    "               s=250, linewidths=2.0, marker='o', zorder=1e3+20*id+0)\n",
    "    ax.annotate(str(id), (mean_x, mean_y),\n",
    "            xytext=(0, 0), color='black',\n",
    "            textcoords='offset points',\n",
    "            ha='center', va='center', fontsize=10, zorder=1e3+20*id+1)\n",
    "\n",
    "# plot legend\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "legend = ax.legend(handles[::], labels[::], loc='upper center',\n",
    "          bbox_to_anchor=(0.5, -0.05), frameon=False, fontsize=8.5, markerscale=2, ncol=2)\n",
    "for handle in legend.legend_handles:\n",
    "    handle.set_alpha(1.0)\n",
    "\n",
    "\n",
    "plt.savefig(plot_dir + 'fig_S1_topics_flat.pdf',bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc2ca68-43db-41dc-8859-87fee3c2769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIGURE S4: PLOT CITATION RATE VS YEAR\n",
    "\n",
    "deep_cluster_id = df['deep_topic_id'].unique()\n",
    "deep_cluster_id = deep_cluster_id[ [2, 0, 1] ]    # manually re-order to match Cluster 1, 2, 3 elsewhere\n",
    "num_clusters = len(deep_cluster_id)\n",
    "\n",
    "df_2024 = df[ (df['TimesFraction'] <= 2024) ]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2)\n",
    "fig.set_size_inches(9,9)\n",
    "fig.tight_layout(pad=1)\n",
    "\n",
    "colors = list( my_colors.values() )\n",
    "\n",
    "plot_pos = [ [0,0], [0,1], [1,0] ]\n",
    "epsilon = 1e-3\n",
    "for i,id in enumerate(deep_cluster_id):\n",
    "    ii = plot_pos[i][0]\n",
    "    jj = plot_pos[i][1]\n",
    "\n",
    "    df_2024_id = df_2024[ df_2024['deep_topic_id'] == id ]\n",
    "    ax[ii,jj].scatter(x=df_2024_id['TimesFraction'],\n",
    "                  y=np.log10(df_2024_id['CitationsPerYear']+epsilon),\n",
    "                  facecolors='none',\n",
    "                  edgecolors=colors[i],\n",
    "                  s=10,\n",
    "                  label=f'Cluster {i+1}',\n",
    "                  alpha=.5,\n",
    "                  zorder=0)\n",
    "\n",
    "    ax[1,1].scatter(x=df_2024_id['TimesFraction'],\n",
    "              y=np.log10(df_2024_id['CitationsPerYear']+epsilon),\n",
    "              facecolors='none',\n",
    "              edgecolors=colors[i],\n",
    "              s=10,\n",
    "              label=f'Cluster {i+1}',\n",
    "              alpha=.5,\n",
    "              zorder=-i)    \n",
    "    \n",
    "    ax[ii,jj].set_title(f'Cluster {i+1} research articles')\n",
    "    ax[ii,jj].set_xticks([1950,1960,1970,1980,1990,2000,2010,2020])\n",
    "    ax[ii,jj].set_yticks([-3,-2,-1,0,1,2,3,4])\n",
    "    \n",
    "    if jj == 0:\n",
    "        #ax[ii,jj].set_ylabel('Citations per year')\n",
    "        ax[ii,jj].set_yticklabels([0.001,0.01,0.1,1,10,100,1000,10000])\n",
    "    else:\n",
    "        ax[ii,jj].set_ylabel('')\n",
    "        ax[ii,jj].set_yticklabels(['']*8)\n",
    "\n",
    "    if ii == 0:\n",
    "        ax[ii,jj].set_xlabel('')\n",
    "        ax[ii,jj].set_xticklabels(['']*8)\n",
    "        \n",
    "ax[1,1].set_title(f'All research articles')\n",
    "ax[1,1].set_yticklabels(['']*8)\n",
    "ax[1,1].set_yticks([-3,-2,-1,0,1,2,3,4])\n",
    "ax[1,1].set_xticks([1950,1960,1970,1980,1990,2000,2010,2020])\n",
    "\n",
    "plt.gcf().subplots_adjust(bottom=0.05)\n",
    "plt.gcf().subplots_adjust(left=0.1)\n",
    "\n",
    "fig.supxlabel('Year', y=-0.01)\n",
    "fig.supylabel('Citation rate (num. citations per year)', x=-0.01)\n",
    "\n",
    "plt.savefig(plot_dir + 'fig_S4_citation_rate_vs_year.pdf',bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f665006-2247-4f7a-a298-b4e8a95cf4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIGURE S5: PLOT RECENT CITATION RATES FOR ARTICLES FROM 2000-2024\n",
    "\n",
    "df_2020_to_2024 = df[ (df['TimesFraction'] <= 2024) & (df['TimesFraction'] > 2000) ]\n",
    "\n",
    "\n",
    "colors = list( my_colors.values() )\n",
    "\n",
    "x1 = np.linspace(-4, 4, 200)\n",
    "\n",
    "qu = 0.99\n",
    "ql = 0.01\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for i,id in enumerate(deep_cluster_id):\n",
    "    df_id = df_2020_to_2024[ df_2020_to_2024['deep_topic_id'] == id ]\n",
    "\n",
    "    # raw values\n",
    "    vv = df_id['CitationsPerYear']\n",
    "    vv = vv[ ~np.isnan(vv) ]\n",
    "    xx = np.log10(vv + 0.001)\n",
    "    xx = xx[ ~np.isnan(xx) ]\n",
    "    dd = gaussian_kde(xx)\n",
    "\n",
    "    # stats\n",
    "    m = np.nanmean(vv)\n",
    "    md = np.nanmedian(vv)\n",
    "    s = np.nanstd(vv)\n",
    "    qql, qqu = np.nanquantile(vv, [ql,qu])\n",
    "    #qql = np.nanquantile(vv, ql)\n",
    "    \n",
    "    print(f'Cluster {i+1}:  ', 'm=', m, 'md=', md, 's=', s, 'q=', qql, ',', qqu)\n",
    "\n",
    "    plt.plot(x1,dd(x1), color=colors[i], label=f\"Cluster {i+1}\")\n",
    "    ax.hlines(-0.02 - 0.04*i, xmin=np.log10(qql), xmax=np.log10(qqu), color=colors[i], lw=2)\n",
    "    ax.plot(np.log10(md), -0.02 - 0.04*i, 'o', color=colors[i], markeredgecolor='black', zorder=5)\n",
    "    ax.plot(np.log10(m), -0.02 - 0.04*i, 's', color='white', markeredgecolor=colors[i], zorder=5)\n",
    "\n",
    "\n",
    "  \n",
    "plt.xticks([-2,-1,0,1,2,3,4])\n",
    "ax.set_xticklabels([0.01,0.1,1,10,100,1000,10000])\n",
    "plt.legend()\n",
    "plt.xlabel(\"Citations per year\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlim(-2,3.5)\n",
    "plt.savefig(plot_dir + \"fig_S5_citation_rate_density.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90267576-300f-4d49-b1d6-00aad3c3cdb3",
   "metadata": {},
   "source": [
    "## Topic modeling (other fun stuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d2f266-4767-4875-bd47-7bd6880b3efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show weighting of keywords for each topic\n",
    "fig = topic_model.visualize_barchart(n_words=10,top_n_topics=50)\n",
    "fig.write_image(plot_dir + \"fig_extra_topic_barchart.pdf\", format=\"pdf\", height=11*300, width=8.5*300)  # requires kaleido\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe996ab-2e00-4537-ba82-37878b10201e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show wordcloud for each topic\n",
    "\n",
    "ncols = 3\n",
    "nrows = int( np.ceil(num_topics / 3) )\n",
    "width = 7.5\n",
    "height = width / ncols * nrows * 0.75\n",
    "\n",
    "fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(width,height))\n",
    "\n",
    "# reindex cluster ids\n",
    "topic_cluster = [0] * num_topics\n",
    "cluster_id = { 0:3, 1:2, 2:1 }\n",
    "for i,v in enumerate(deep_topic_id.values()):\n",
    "    for j in v:\n",
    "        topic_cluster[j] = cluster_id[i]\n",
    "\n",
    "# make word clouds\n",
    "topic_id = 0\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        text = {word: value for word, value in topic_model.get_topic(topic_id)}\n",
    "        wc = WordCloud(background_color=\"white\", max_words=1000)\n",
    "        wc.generate_from_frequencies(text)\n",
    "        ax[i,j].imshow(wc, interpolation=\"bilinear\")\n",
    "        ax[i,j].axis(\"off\")\n",
    "        ax[i,j].set_title(f'Topic {topic_id} (cluster {topic_cluster[topic_id]})')\n",
    "        topic_id += 1\n",
    "\n",
    "plt.tight_layout(pad=1.5)\n",
    "plt.savefig(plot_dir + \"fig_extra_topic_wordclouds.pdf\")\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b84798-d6be-46fe-b32f-b018c15b1f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show similarity matrix among topics\n",
    "size = 1000\n",
    "fig = topic_model.visualize_heatmap(n_clusters=6,height=size,width=size)\n",
    "fig.write_image(plot_dir + \"fig_extra_topic_heatmap.pdf\", format=\"pdf\", height=7.5, width=7.5)  # requires kaleido\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8d05cd-4fda-4855-9527-47ba283b36c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful to inspect words in a specific article (text) contribute to different topic assignments\n",
    "paper_idx = 100\n",
    "print(f'Inspecting article with index {paper_idx}:')\n",
    "print('Article text:')\n",
    "print(texts[paper_idx])\n",
    "\n",
    "# get topic probabilities\n",
    "fig1 = topic_model.visualize_distribution(topic_distr[paper_idx])\n",
    "print('Topic probabilities:')\n",
    "fig1.show()\n",
    "\n",
    "# get topic probs per token\n",
    "fig2 = topic_model.visualize_approximate_distribution(texts[paper_idx], topic_token_distr[paper_idx])\n",
    "print('Topic probabilities per token:')\n",
    "fig2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6d26b9-7a17-4757-8948-3021d6e11373",
   "metadata": {},
   "source": [
    "## Similarities (processing & visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c12c10b-3643-4c21-89a7-3b938dfa2f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESS SIMILARITIES\n",
    "\n",
    "# NOTE: We use the embeddings from much, much earlier!\n",
    "\n",
    "# compute similarities between all pairs of articles \n",
    "similarities = np.zeros([n_texts,n_texts])\n",
    "for i in range(0,n_texts):\n",
    "    for j in range(i,n_texts):\n",
    "        # similarities are symmetric\n",
    "        similarities[i][j] = cosine_similarity(embeddings[i], embeddings[j])\n",
    "        similarities[j][i] = similarities[i][j]\n",
    "\n",
    "# build matrix of mean similarity across year-bins\n",
    "yy = np.unique(years)\n",
    "n_years = len(yy)\n",
    "mean_sim = np.zeros((n_years,n_years))\n",
    "for i in range(n_years):\n",
    "    ii = np.where(years == yy[i])\n",
    "    for j in range(n_years):\n",
    "        jj = np.where(years == yy[j])\n",
    "        mm = np.mean( similarities[ ii[0][0]:ii[0][-1], jj[0][0]:jj[0][-1] ] )\n",
    "        mean_sim[i,j] = mm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11890a0a-35c0-4a78-b909-d05ecf238d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIGURE 1: HEATMAP OF MEAN-SIMILARITIES BY YEAR\n",
    "\n",
    "# colors\n",
    "n_bins = 5\n",
    "hex_colors = ['#5d75c9', '#8da5da', '#b3c5e6', '#d5def0', '#eef2fb'][::-1]\n",
    "cmap = matplotlib.colors.ListedColormap(hex_colors)\n",
    "\n",
    "# heatmap\n",
    "hm = plt.imshow(mean_sim, cmap=cmap, interpolation='nearest')\n",
    "\n",
    "# axes\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel(\"Journal year\")\n",
    "plt.ylabel(\"Journal year\")\n",
    "xy_ticks = list(range(n_years))[::10]\n",
    "xy_lbls = [str(x) for x in yy][::10]\n",
    "plt.xticks(xy_ticks, xy_lbls)\n",
    "plt.yticks(xy_ticks, xy_lbls)\n",
    "\n",
    "# legend\n",
    "cbar = plt.colorbar(hm)\n",
    "cbar.set_label('mean similarity')\n",
    "\n",
    "# save\n",
    "plt.savefig(plot_dir + 'fig_1_heatmap_similarity.pdf')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8045c4-112c-4056-b3b0-5c22de3ce382",
   "metadata": {},
   "source": [
    "## Article types (processing & visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6918bc-8887-4a7f-889e-3cc7e9c3fb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather article statistics vs year\n",
    "tmp_df = {\n",
    "    'pov_len':  df_orig[ df_orig['Document.Type.Curated']=='Point Of View'   ].groupby('Volume')['PageLength'].mean(),\n",
    "    'art_len':  df_orig[ df_orig['Document.Type.Curated']=='Research Article'].groupby('Volume')['PageLength'].mean(),\n",
    "    'book_len': df_orig[ df_orig['Document.Type.Curated']=='Book Review'     ].groupby('Volume')['PageLength'].mean(),\n",
    "    'pov_num':  df_orig[ df_orig['Document.Type.Curated']=='Point Of View'   ].groupby('Volume')['PageLength'].count(),\n",
    "    'art_num':  df_orig[ df_orig['Document.Type.Curated']=='Research Article'].groupby('Volume')['PageLength'].count(),\n",
    "    'book_num': df_orig[ df_orig['Document.Type.Curated']=='Book Review'     ].groupby('Volume')['PageLength'].count()\n",
    "}\n",
    "\n",
    "# get moving averages of counts/mean-lengths\n",
    "y_df = {}\n",
    "volumes = list(range(1,76))\n",
    "for k,v in tmp_df.items():\n",
    "    y = np.zeros( len(volumes) )\n",
    "    x_tmp = [ int(z-1) for z in v.axes[0].tolist()]\n",
    "    y_tmp = v.tolist()\n",
    "    y[x_tmp] = moving_average( y_tmp, 1)\n",
    "    y_df[k] = y\n",
    "\n",
    "# 1952 to 2024 (vols 1-73)\n",
    "max_vol = 73\n",
    "volumes = volumes[:max_vol]\n",
    "for k,v in y_df.items():\n",
    "    y_df[k] = v[:max_vol]\n",
    "    \n",
    "years = 2025 - np.array(volumes) - 1\n",
    "years = years[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2caced-2544-4aac-a37b-d6dbe7f02a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIGURE 3: NUMBER/LENGTH OF ARTICLE VS YEAR\n",
    "\n",
    "fig, ax = plt.subplots( nrows=1, ncols=2)\n",
    "fig.set_size_inches(10, 5)\n",
    "\n",
    "colors = {\n",
    "    'art':  '#a044b2',\n",
    "    'pov':  '#2a9d6f',\n",
    "    'book': '#e1a95f',\n",
    "}\n",
    "lw = 1.25\n",
    "\n",
    "# number of articles\n",
    "ax[0].plot(years, y_df['art_num'], c=colors['art'], label='Research Article', lw=lw)\n",
    "ax[0].plot(years, y_df['pov_num'], c=colors['pov'], label='Point of View', lw=lw)\n",
    "ax[0].plot(years, y_df['book_num'], c=colors['book'], label='Book Review', lw=lw)\n",
    "ax[0].set_xlabel('Year')\n",
    "ax[0].set_ylabel('Num. articles')\n",
    "ax[0].legend()\n",
    "\n",
    "# number of articles\n",
    "ax[1].plot(years, y_df['art_len'], c=colors['art'], label='Research Article', lw=lw)\n",
    "ax[1].plot(years, y_df['pov_len'], c=colors['pov'], label='Point of View', lw=lw)\n",
    "ax[1].plot(years, y_df['book_len'], c=colors['book'], label='Book Review', lw=lw)\n",
    "ax[1].set_xlabel('Year')\n",
    "ax[1].set_ylabel('Mean article length')\n",
    "ax[1].legend()\n",
    "\n",
    "# save\n",
    "plt.savefig(plot_dir + 'article_types_thru_time.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
